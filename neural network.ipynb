{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "498d43bd",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aefb485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdcff0b",
   "metadata": {},
   "source": [
    "# Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639520c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_layers: List[int]) -> None:\n",
    "        # Initialize the neural network with the given parameters.\n",
    "\n",
    "        self.input_size: int = input_size\n",
    "        self.output_size: int = output_size\n",
    "        self.hidden_layers: List[int] = hidden_layers\n",
    "\n",
    "        # Create a complete list of layer sizes including input and output layers\n",
    "        layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights: List[np.ndarray] = []\n",
    "        self.biases: List[np.ndarray] = []\n",
    "\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * 0.01)\n",
    "            self.biases.append(np.zeros((1, layer_sizes[i + 1])))  \n",
    "      \n",
    "        \n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, epochs: int, learning_rate: float, loss_threshold: float = None) -> None:\n",
    "        # Train the neural network using stochastic gradient descent with early stopping and data shuffling.\n",
    "        \n",
    "        num_samples = X.shape[0]\n",
    "        total_iterations = epochs * num_samples      \n",
    "        with tqdm(total=total_iterations, desc=\"Training Progress\") as pbar:\n",
    "            for epoch in range(epochs):\n",
    "                # Shuffle data at the beginning of each epoch\n",
    "                indices = np.arange(num_samples)\n",
    "                np.random.shuffle(indices)\n",
    "                X = X[indices]\n",
    "                y = y[indices]\n",
    "    \n",
    "                epoch_loss = 0\n",
    "                for i in range(num_samples):\n",
    "                    x = X[i]\n",
    "                    target = y[i]\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    activations = self.forward(x)\n",
    "                    y_pred = activations[-1]\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    loss = self.compute_loss(target, y_pred)\n",
    "                    epoch_loss += loss\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    grads_w, grads_b = self.backward(activations, target)\n",
    "                    \n",
    "                    # Update parameters\n",
    "                    self.update_parameters(grads_w, grads_b, learning_rate)\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    pbar.update(1)\n",
    "                \n",
    "                avg_loss = epoch_loss / num_samples\n",
    "                #print(f\"epoch_loss / num_samples: {epoch_loss:.1f}/{num_samples} = avg_loss: {avg_loss}\")\n",
    "                #print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss}\")\n",
    "    \n",
    "                # Early stopping based on average training loss\n",
    "                if loss_threshold is not None and avg_loss < loss_threshold:\n",
    "                    print(f\"Early stopping at epoch {epoch+1} due to avg_loss < {loss_threshold}\")\n",
    "                    pbar.n = pbar.total  # Set progress bar to its maximum value\n",
    "                    pbar.close()  # Close the progress bar\n",
    "                    return  # Use return to exit the function instead of break\n",
    "        \n",
    "        print(\"Training complete.\")\n",
    "        pbar.close()\n",
    "    \n",
    "            \n",
    "    def forward(self, x: np.ndarray) -> List[np.ndarray]:\n",
    "        # Perform a forward pass through the network to compute activations for a single sample.\n",
    " \n",
    "        activations = [x]\n",
    "        current_active_layer = x\n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            z = np.dot(current_active_layer, self.weights[i]) + self.biases[i]\n",
    "            current_active_layer = self.relu(z)\n",
    "            activations.append(current_active_layer)\n",
    "        \n",
    "        z = np.dot(current_active_layer, self.weights[-1]) + self.biases[-1]\n",
    "        current_active_layer = self.softmax(z)\n",
    "        activations.append(current_active_layer)\n",
    "        \n",
    "        return activations\n",
    "\n",
    "    \n",
    "    def compute_loss(self, y: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        # Compute the categorical cross-entropy loss.\n",
    "       \n",
    "        # Adding a small value (1e-8) to y_pred to avoid taking the log of zero.\n",
    "        loss = -np.sum(y * np.log(y_pred + 1e-8))\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def compute_output_gradients(self, y: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        # Compute the gradient of the loss with respect to the output of the network (softmax).\n",
    "\n",
    "        return y_pred - y\n",
    "    \n",
    "    \n",
    "    def backward(self, activations: List[np.ndarray], y: np.ndarray) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "        # Perform the backward pass to compute gradients for weights and biases.\n",
    "\n",
    "        grads_w = [None] * len(self.weights)\n",
    "        grads_b = [None] * len(self.biases)\n",
    "        \n",
    "        # Compute gradient for the output layer\n",
    "        delta = self.compute_output_gradients(y, activations[-1])\n",
    "        grads_w[-1] = np.outer(activations[-2], delta)\n",
    "        grads_b[-1] = delta\n",
    "        \n",
    "        # Backpropagate through hidden layers\n",
    "        for i in range(len(self.hidden_layers)-1, -1, -1):\n",
    "            delta = np.dot(delta, self.weights[i+1].T) * (activations[i+1] > 0)  # Gradient of ReLU\n",
    "            grads_w[i] = np.outer(activations[i], delta)\n",
    "            grads_b[i] = delta\n",
    "            \n",
    "        return grads_w, grads_b\n",
    "    \n",
    "    \n",
    "    def update_parameters(self, grads_w: List[np.ndarray], grads_b: List[np.ndarray], learning_rate: float) -> None:\n",
    "        # Update weights and biases using computed gradients and learning rate.\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * grads_w[i]\n",
    "            self.biases[i] -= learning_rate * grads_b[i]\n",
    "    \n",
    "    \n",
    "    def relu(self, z: np.ndarray) -> np.ndarray:\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    \n",
    "    def softmax(self, z: np.ndarray) -> np.ndarray:\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        # Perform a forward pass through the network to make predictions.\n",
    "\n",
    "        a = X\n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            z = np.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = self.relu(z)\n",
    "        \n",
    "        z = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        output = self.softmax(z)\n",
    "        return output\n",
    "\n",
    "    \n",
    "    def score(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        # Calculate the accuracy of the neural network on the given dataset.\n",
    " \n",
    "        # Perform predictions on the entire dataset\n",
    "        y_pred = self.predict(X)\n",
    "\n",
    "        # Calculate the number of correct predictions\n",
    "        correct_predictions = np.sum(np.argmax(y_pred, axis=1) == np.argmax(y, axis=1))\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = correct_predictions / X.shape[0]\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03033a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(X_train, y_train, X_val, y_val, hidden_layers_options, learning_rates, epochs_options, loss_threshold):\n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "\n",
    "    # Sort epochs_options to ensure incremental training\n",
    "    epochs_options = sorted(epochs_options)\n",
    "\n",
    "    # Iterate over all combinations of hyperparameters\n",
    "    for hidden_layers, learning_rate in product(hidden_layers_options, learning_rates):\n",
    "\n",
    "        nn = NeuralNetwork(input_size=X_train.shape[1], output_size=num_classes, hidden_layers=hidden_layers)\n",
    "        \n",
    "        print(f\"Testing with hidden_layers={hidden_layers}, {learning_rate=}, {epochs_options=}\")\n",
    "        \n",
    "        for i, epochs in enumerate(epochs_options):\n",
    "            additional_epochs = epochs if i == 0 else epochs - epochs_options[i-1]\n",
    "\n",
    "            # Continue training the neural network\n",
    "            nn.fit(X_train, y_train, epochs=additional_epochs, learning_rate=learning_rate, loss_threshold=loss_threshold)\n",
    "\n",
    "            # Validate the neural network\n",
    "            val_accuracy = nn.score(X_val, y_val)\n",
    "            print(f\"Validation Accuracy after {epochs} epochs: {val_accuracy * 100:.2f}%\\n\\n\")\n",
    "\n",
    "            # Update best parameters if current model is better\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "                best_params = (hidden_layers, learning_rate, epochs)\n",
    "    print(f\"Best Validation Accuracy: {best_accuracy * 100:.2f}% with parameters: {best_params}\")\n",
    "    return best_params, best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aece32",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3ee380",
   "metadata": {},
   "source": [
    "### preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee43b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST data\n",
    "MNIST_train = pd.read_csv(\"csv files/MNIST-train.csv\")\n",
    "\n",
    "# Split the column named 'y' from the rest of the columns\n",
    "y = MNIST_train['y']\n",
    "X = MNIST_train.drop(columns=['y'])\n",
    "\n",
    "# Normalize the input data\n",
    "X = X / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = 10\n",
    "y_one_hot = np.zeros((y.size, num_classes))\n",
    "y_one_hot[np.arange(y.size), y] = 1\n",
    "\n",
    "# Use train_test_split to split the data into training and validation sets\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab2a07d",
   "metadata": {},
   "source": [
    "## A little warm-up to see that the model is actually working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf151da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the neural network object\n",
    "n = X_train.shape[1]\n",
    "output_size = num_classes\n",
    "hidden_layers = [10]\n",
    "\n",
    "nn = NeuralNetwork(input_size=n, output_size=output_size, hidden_layers=hidden_layers)\n",
    "\n",
    "# Train the neural network\n",
    "nn.fit(X_train.values, y_train, epochs=1, learning_rate=0.01,loss_threshold=0.1)\n",
    "\n",
    "# Test the neural network on the validation data\n",
    "accuracy = nn.score(X_validation.values, y_validation)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcdda05",
   "metadata": {},
   "source": [
    "## Let's find the best hyperparameters using a grid search!\n",
    "### Beware! It took over 8 hours to complete...\n",
    "### I copied the resaults to a file in named MNIST_grid for your convenience\n",
    "### run this if you want to find a good set of parameters, once found there is no need to search again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db9b92a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "seems like going from 784 layers to 10 is a bit of a strech\n",
    "and yet we are getting a resault of arounf 90%\n",
    "let's try a grid search to find the best hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "hidden_layers_options = [\n",
    "    [64], [128], [150],[200],[250],\n",
    "    [64, 32], [128, 64], [200, 100],\n",
    "    [64, 64], [128, 128],[200, 50],\n",
    "]\n",
    "learning_rates = [0.05,0.01, 0.015, 0.02]\n",
    "epochs_options = [10, 20, 50]\n",
    "loss_threshold = 0.000001\n",
    "\n",
    "\n",
    "# only run when looking for hyper parameters\n",
    "# best_params, best_accuracy = grid_search(X_train.values, y_train, X_validation.values, y_validation, hidden_layers_options, learning_rates, epochs_options, loss_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6bf61a",
   "metadata": {},
   "source": [
    "##  We have found what looks to be good hyperparameters! \n",
    "### Now let's check the accuracy on the test set.\n",
    "### But before we do, let's retrain the Neural Network using the validation set to make it even better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46a23fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST data\n",
    "MNIST_train = pd.read_csv(\"csv files/MNIST-train.csv\")\n",
    "MNIST_test = pd.read_csv(\"csv files/MNIST-train.csv\")\n",
    "\n",
    "# Split the column named 'y' from the rest of the columns\n",
    "y_train = MNIST_train['y']\n",
    "X_train = MNIST_train.drop(columns=['y'])\n",
    "\n",
    "y_test = MNIST_test['y']\n",
    "X_test = MNIST_test.drop(columns=['y'])\n",
    "\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = 10\n",
    "y_train_one_hot = np.zeros((y_train.size, num_classes))\n",
    "y_train_one_hot[np.arange(y_train.size), y] = 1\n",
    "\n",
    "y_test_one_hot = np.zeros((y_test.size, num_classes))\n",
    "y_test_one_hot[np.arange(y_test.size), y_test] = 1\n",
    "\n",
    "# Normalize the input data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "best_params = ([100],0.02,10) # hard coded so the tester doesnt need to run the earch again \n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f107f5c",
   "metadata": {},
   "source": [
    "## Let's see what resault we get with the best hyperparameters we found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e828b4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the best model on the entire training data (X and y_one_hot)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m best_hidden_layers, best_learning_rate, best_epochs \u001b[38;5;241m=\u001b[39m best_params\n\u001b[0;32m      4\u001b[0m best_nn \u001b[38;5;241m=\u001b[39m NeuralNetwork(input_size\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], output_size\u001b[38;5;241m=\u001b[39mnum_classes, hidden_layers\u001b[38;5;241m=\u001b[39mbest_hidden_layers)\n\u001b[0;32m      5\u001b[0m best_nn\u001b[38;5;241m.\u001b[39mfit(X_train\u001b[38;5;241m.\u001b[39mvalues, y_train_one_hot, epochs\u001b[38;5;241m=\u001b[39mbest_epochs, learning_rate\u001b[38;5;241m=\u001b[39mbest_learning_rate, loss_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00001\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_params' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the best model on the entire training data (X and y_one_hot)\n",
    "best_hidden_layers, best_learning_rate, best_epochs = best_params\n",
    "\n",
    "best_nn = NeuralNetwork(input_size=X.shape[1], output_size=num_classes, hidden_layers=best_hidden_layers)\n",
    "best_nn.fit(X_train.values, y_train_one_hot, epochs=best_epochs, learning_rate=best_learning_rate, loss_threshold=0.00001)\n",
    "\n",
    "# Test the best model\n",
    "test_accuracy = best_nn.score(X_test.values, y_test_one_hot)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f'best parameters found: hidden layers: {best_hidden_layers}, learning rate: {best_learning_rate}, best epochs: {best_epochs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c0cbab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
